{
    "sourceFile": "train_kitti.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 6,
            "patches": [
                {
                    "date": 1735525814488,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1735525821423,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,14 +25,10 @@\n parser.add_argument('--maxdisp', type=int, default=192,\n                     help='maxium disparity')\n parser.add_argument('--datapath_kitti2015', default='/data/ywang/dataset/kitti_2015/training/',\n                     help='datapath for sceneflow monkaa dataset')\n-parser.add_argument('--datapath_kitti2015_test', default='/data/ywang/dataset/kitti_2015/testing/',\n-                     help='datapath for sceneflow monkaa dataset')\n parser.add_argument('--datapath_kitti', default='/data/ywang/dataset/kitti_2012/training/',\n                      help='datapath for sceneflow monkaa dataset')\n-parser.add_argument('--datapath_kitti_test', default='/data/ywang/dataset/kitti_2012/testing/',\n-                    help='datapath for sceneflow monkaa dataset')\n \n parser.add_argument('--epochs', type=int, default=800,\n                     help='number of epochs to train')\n parser.add_argument('--loadmodel', default='/home/ywang/my_projects/my/GwcNet-augment/fined/final_kitti.tar',\n"
                },
                {
                    "date": 1735525831838,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,14 +28,14 @@\n                     help='datapath for sceneflow monkaa dataset')\n parser.add_argument('--datapath_kitti', default='/data/ywang/dataset/kitti_2012/training/',\n                      help='datapath for sceneflow monkaa dataset')\n \n-parser.add_argument('--epochs', type=int, default=800,\n+parser.add_argument('--epochs', type=int, default=1000,\n                     help='number of epochs to train')\n parser.add_argument('--loadmodel', default='/home/ywang/my_projects/my/GwcNet-augment/fined/final_kitti.tar',\n                     help='load model')\n parser.add_argument('--gpus', type=int, nargs='+', default=[0])\n-parser.add_argument('--savemodel', default='/home/wy/GwcNet-augment/fined/KITTI15',\n+parser.add_argument('--savemodel', default='/home/wy/DCANet/fined/KITTI15',\n                     help='save model')\n parser.add_argument('--no-cuda', action='store_true', default=False,\n                     help='enables CUDA training')\n parser.add_argument('--seed', type=int, default=1, metavar='S',\n"
                },
                {
                    "date": 1735525838103,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,13 +54,10 @@\n     torch.cuda.manual_seed(args.seed)\n \n all_left_img_0, all_right_img_0, all_left_disp_0,  \\\n test_left_img_0, test_right_img_0, test_left_disp_0 = DA.dataloader_KITTI2015(args.datapath_kitti2015)\n+\n #\n-all_left_img_1, all_right_img_1, all_left_disp_1,  \\\n-test_left_img_1, test_right_img_1, test_left_disp_1 = DA.dataloader_KITTI2015(args.datapath_kitti2015_test)\n-# #\n-#\n all_left_img_2, all_right_img_2, all_left_disp_2,  \\\n test_left_img_2, test_right_img_2, test_left_disp_2 = DA.dataloader_KITTI(args.datapath_kitti)\n # #\n all_left_img_3, all_right_img_3, all_left_disp_3,  \\\n"
                },
                {
                    "date": 1735525843933,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -62,10 +62,8 @@\n # #\n all_left_img_3, all_right_img_3, all_left_disp_3,  \\\n test_left_img_3, test_right_img_3, test_left_disp_3 = DA.dataloader_KITTI(args.datapath_kitti_test)\n \n-all_left_img_4, all_right_img_4, all_left_disp_4,  \\\n-test_left_img_4, test_right_img_4, test_left_disp_4 = DA.dataloader_KITTI2015_test(args.datapath_kitti2015)\n \n all_left_img = all_left_img_0 + all_left_img_1 + all_left_img_2\n all_right_img = all_right_img_0 + all_right_img_1 + all_right_img_2\n all_left_disp = all_left_disp_0 + all_left_disp_1 + all_left_disp_2\n"
                },
                {
                    "date": 1735525851506,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n test_left_img_3, test_right_img_3, test_left_disp_3 = DA.dataloader_KITTI(args.datapath_kitti_test)\n \n \n all_left_img = all_left_img_0 + all_left_img_1 \n-all_right_img = all_right_img_0 + all_right_img_1 + all_right_img_2\n+all_right_img = all_right_img_0 + all_right_img_1\n all_left_disp = all_left_disp_0 + all_left_disp_1 + all_left_disp_2\n \n TrainImgLoader = torch.utils.data.DataLoader(\n     DA.myImageFloder_KITTI(all_left_img, all_right_img, all_left_disp, True),\n"
                },
                {
                    "date": 1735525865375,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -56,9 +56,9 @@\n all_left_img_0, all_right_img_0, all_left_disp_0,  \\\n test_left_img_0, test_right_img_0, test_left_disp_0 = DA.dataloader_KITTI2015(args.datapath_kitti2015)\n #\n all_left_img_1, all_right_img_1, all_left_disp_1,  \\\n-test_left_img_1, test_right_img_2, test_left_disp_2 = DA.dataloader_KITTI(args.datapath_kitti)\n+test_left_img_1, test_right_img_1, test_left_disp_2 = DA.dataloader_KITTI(args.datapath_kitti)\n \n all_left_img = all_left_img_0 + all_left_img_1 \n all_right_img = all_right_img_0 + all_right_img_1\n all_left_disp = all_left_disp_0 + all_left_disp_1 \n"
                }
            ],
            "date": 1735525814488,
            "name": "Commit-0",
            "content": "from __future__ import print_function\nimport sys\nsys.path.append(\"dataloader\")\nfrom torch.autograd import Variable\nfrom models.gwcnet_dca_g import *\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom util import *\nimport time\nimport dataloader.datasets as DA\nimport dataloader.KITTILoader as lt\n\nimport os\nfrom models.loss import StereoFocalLoss, model_loss\nimport torch.backends.cudnn as cudnn\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nparser = argparse.ArgumentParser(description='GwcNet')\nparser.add_argument('--maxdisp', type=int, default=192,\n                    help='maxium disparity')\nparser.add_argument('--datapath_kitti2015', default='/data/ywang/dataset/kitti_2015/training/',\n                    help='datapath for sceneflow monkaa dataset')\nparser.add_argument('--datapath_kitti2015_test', default='/data/ywang/dataset/kitti_2015/testing/',\n                     help='datapath for sceneflow monkaa dataset')\nparser.add_argument('--datapath_kitti', default='/data/ywang/dataset/kitti_2012/training/',\n                     help='datapath for sceneflow monkaa dataset')\nparser.add_argument('--datapath_kitti_test', default='/data/ywang/dataset/kitti_2012/testing/',\n                    help='datapath for sceneflow monkaa dataset')\n\nparser.add_argument('--epochs', type=int, default=800,\n                    help='number of epochs to train')\nparser.add_argument('--loadmodel', default='/home/ywang/my_projects/my/GwcNet-augment/fined/final_kitti.tar',\n                    help='load model')\nparser.add_argument('--gpus', type=int, nargs='+', default=[0])\nparser.add_argument('--savemodel', default='/home/wy/GwcNet-augment/fined/KITTI15',\n                    help='save model')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='enables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--print_freq', type=int, default=1, help='the frequency of printing losses (iterations)')\nparser.add_argument('--lrepochs', type=str, default=\"200:2\", help='the epochs to decay lr: the downscale rate')\nparser.add_argument('--lr', type=float, default=1e-3, help='initial learning rate')\nparser.add_argument('--focal_coefficient', type=float, default=5.0,  help='initial learning rate')\nparser.add_argument('--sparse', type=bool, default=False, help='initial learning rate')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nall_left_img_0, all_right_img_0, all_left_disp_0,  \\\ntest_left_img_0, test_right_img_0, test_left_disp_0 = DA.dataloader_KITTI2015(args.datapath_kitti2015)\n#\nall_left_img_1, all_right_img_1, all_left_disp_1,  \\\ntest_left_img_1, test_right_img_1, test_left_disp_1 = DA.dataloader_KITTI2015(args.datapath_kitti2015_test)\n# #\n#\nall_left_img_2, all_right_img_2, all_left_disp_2,  \\\ntest_left_img_2, test_right_img_2, test_left_disp_2 = DA.dataloader_KITTI(args.datapath_kitti)\n# #\nall_left_img_3, all_right_img_3, all_left_disp_3,  \\\ntest_left_img_3, test_right_img_3, test_left_disp_3 = DA.dataloader_KITTI(args.datapath_kitti_test)\n\nall_left_img_4, all_right_img_4, all_left_disp_4,  \\\ntest_left_img_4, test_right_img_4, test_left_disp_4 = DA.dataloader_KITTI2015_test(args.datapath_kitti2015)\n\nall_left_img = all_left_img_0 + all_left_img_1 + all_left_img_2\nall_right_img = all_right_img_0 + all_right_img_1 + all_right_img_2\nall_left_disp = all_left_disp_0 + all_left_disp_1 + all_left_disp_2\n\nTrainImgLoader = torch.utils.data.DataLoader(\n    DA.myImageFloder_KITTI(all_left_img, all_right_img, all_left_disp, True),\n    batch_size=1, shuffle=True, num_workers=4, drop_last=False)\n\nTestImgLoader = torch.utils.data.DataLoader(\n    DA.myImageFloder_KITTI(test_left_img_1, test_right_img_1, test_left_disp_1, False),\n    batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n\nmodel = GwcNet(args.maxdisp)\nif args.cuda:\n    model = nn.DataParallel(model)\n    model.cuda()\n\nif args.loadmodel is not None:\n    print('Load pretrained model')\n    state_dict = torch.load(args.loadmodel)\n    from collections import OrderedDict\n    model_state_dict = OrderedDict()\n\n    for k, v in state_dict['state_dict'].items():\n        k = k.replace('module.', '')\n        model_state_dict[k] = v\n    model.load_state_dict(state_dict['state_dict'])\n\nprint('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n\ndef train(imgL, imgR, disp_true):\n    model.train()\n    imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_true.cuda()\n    # ---------\n    mask = ((disp_true < 192) & (disp_true > 0)).byte().bool()\n    mask.detach_()\n    # ----\n    optimizer.zero_grad()\n\n    focal_loss_evaluator = \\\n        StereoFocalLoss(max_disp=args.maxdisp, focal_coefficient=args.focal_coefficient, sparse=args.sparse)\n\n    cost_au_0, cost_au_1, disp_ests = model(imgL, imgR)\n    #\n    # output1 = torch.squeeze(output1, 1)\n    # output2 = torch.squeeze(output2, 1)\n    # output3 = torch.squeeze(output3, 1)\n\n    CE_loss = 5 * focal_loss_evaluator(cost_au_0, disp_true, variance=1) + 10 * focal_loss_evaluator(cost_au_1, disp_true, variance=1)\n\n    loss_disp = model_loss(disp_ests, disp_true, mask)\n    loss = loss_disp + CE_loss\n    epe = torch.mean(torch.abs(disp_ests[-1][mask] - disp_true[mask]))\n    # print('ce_loss:%.3f, output3:%.3f'%(tensor2float(5*CE_loss), tensor2float(loss_disp)))\n\n    loss.backward()\n    optimizer.step()\n\n    return loss.item(), epe.item()\n\ndef mytest(imgL, imgR, disp_true):\n    model.eval()\n    imgL = Variable(torch.FloatTensor(imgL))\n    imgR = Variable(torch.FloatTensor(imgR))\n    if args.cuda:\n        imgL, imgR = imgL.cuda(), imgR.cuda()\n\n    mask = (disp_true > 0) & (disp_true < args.maxdisp)\n\n    if imgL.shape[2] % 16 != 0:\n        times = imgL.shape[2] // 16\n        top_pad = (times + 1) * 16 - imgL.shape[2]\n    else:\n        top_pad = 0\n\n    if imgL.shape[3] % 16 != 0:\n        times = imgL.shape[3] // 16\n        right_pad = (times + 1) * 16 - imgL.shape[3]\n    else:\n        right_pad = 0\n\n    imgL = F.pad(imgL, (0, right_pad, top_pad, 0))\n    imgR = F.pad(imgR, (0, right_pad, top_pad, 0))\n\n    with torch.no_grad():\n        output3 = model(imgL, imgR)\n    if top_pad != 0:\n        pred_disp = output3[:, top_pad:, :]\n    else:\n        pred_disp = output3\n    pred_disp = pred_disp.data.cpu()\n\n    if len(disp_true[mask]) == 0:\n        loss = 0\n        epe = 0\n    else:\n        loss = F.smooth_l1_loss(pred_disp[mask], disp_true[mask], size_average=True)\n        # epe = torch.mean(torch.abs(pred_disp[mask]-disp_true[mask]))  # end-point-error\n        epe = F.l1_loss(pred_disp[mask], disp_true[mask], size_average=True)\n    return loss, epe\n\nprint(\"Traindataset is %d\"%len(TrainImgLoader))\nprint(\"Testdataset is %d\"%len(TestImgLoader))\n\ndef main():\n    for epoch in range(1, args.epochs):\n        print('This is %d-th epoch' % (epoch))\n        total_train_loss = 0.0\n        total_train_epe = 0.0\n        learning_rate_adjust(optimizer, epoch)\n\n        # ## training ##\n        # for batch_idx, (imgL_crop, imgR_crop, disp_crop_L) in enumerate(TrainImgLoader):\n        #\n        #     loss, epe = train(imgL_crop, imgR_crop, disp_crop_L)\n        #     total_train_loss += loss\n        #     total_train_epe += epe\n\n            #if batch_idx % args.print_freq == 0:\n            #    print('### batch_idx %5d of total %5d, loss---%.3f, EPE---%.3f ###' %\n            #          (batch_idx + 1,\n            #           len(TrainImgLoader),\n            #           float(total_train_loss / (batch_idx + 1)),\n            #           float(total_train_epe / (batch_idx + 1))))\n\n        # if (epoch + 1) % 1 == 0:\n        #     avg_epe = total_train_epe / len(TrainImgLoader)\n        #     avg_loss = total_train_loss / len(TrainImgLoader)\n        #     print('Train Epoch----%5d of %d, train_loss---%.3f, train_EPE---%.3f' %\n        #           (epoch + 1, len(TrainImgLoader), avg_loss, avg_epe))\n\n        # SAVE\n        # savefilename = args.savemodel + '/checkpoint_' + str(epoch) + '.tar'\n        # if epoch > 449:\n        #     torch.save({\n        #     'epoch': epoch,\n        #     'state_dict': model.state_dict(),\n        #     'train_loss': total_train_loss / len(TrainImgLoader),\n        # }, savefilename)\n\n        # ------------- TEST ------------------------------------------------------------\n        if epoch > 0:\n            total_test_loss = 0.0\n            total_test_epe = 0.0\n            start_time = time.time()\n            for batch_idx, (imgL, imgR, disp_L) in enumerate(TestImgLoader):\n                test_loss, test_epe = mytest(imgL, imgR, disp_L)\n                total_test_loss += test_loss\n                total_test_epe += test_epe\n                if batch_idx % args.print_freq == 0:\n                    print('### batch_idx %5d of total %5d, loss---%.3f, EPE---%.3f ###' %\n                          (batch_idx + 1,\n                           len(TestImgLoader),\n                           float(total_test_loss / (batch_idx + 1)),\n                           float(total_test_epe / (batch_idx + 1))))\n\n            # print(\"this epoch total time is %.3f\"%(time.time()-start_time))\n            if (epoch + 1) % 1 == 0:\n                avg_epe = total_test_epe / len(TestImgLoader)\n                avg_loss = total_test_loss / len(TestImgLoader)\n                print('Test epoch----%5d pf %d, loss---%.3f, EPE---%.3f' %\n                    (epoch + 1, len(TestImgLoader), avg_loss, avg_epe))\n\n\nif __name__ == '__main__':\n    main()\n\n"
        }
    ]
}